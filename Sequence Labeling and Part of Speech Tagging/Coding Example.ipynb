{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c805a6ea",
   "metadata": {},
   "source": [
    "# The old man the boat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa3b93ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE = \"The old man the boats.\"\n",
    "\n",
    "sentence = SENTENCE.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d37a4",
   "metadata": {},
   "source": [
    "# Hidden Markov Model\n",
    "\n",
    "Let's start by using a hidden markov model. It is implemented in the nltk package. We will train it on the brown dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c33af2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\altzh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\altzh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DET'), ('old', 'ADJ'), ('man', 'NOUN'), ('the', 'DET'), ('boats.', 'DET')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tag\\hmm.py:336: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tag\\hmm.py:334: RuntimeWarning: overflow encountered in cast\n",
      "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tag\\hmm.py:364: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import hmm\n",
    "\n",
    "# Ensure necessary datasets are downloaded\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "# Load tagged sentences from the Brown corpus\n",
    "tagged_sentences = nltk.corpus.brown.tagged_sents(tagset='universal')\n",
    "\n",
    "train_data = tagged_sentences[:5000]\n",
    "\n",
    "# Train the HMM tagger\n",
    "trainer = hmm.HiddenMarkovModelTrainer()\n",
    "hmm_tagger = trainer.train(train_data)\n",
    "\n",
    "# Test the HMM tagger on a new sentence\n",
    "tagged_sentence = hmm_tagger.tag(sentence)\n",
    "\n",
    "print(tagged_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c28c37d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ac15a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:671: FutureWarning: 'cached_download' is the legacy way to download files from the HF hub, please consider upgrading to 'hf_hub_download'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-27 09:46:23,085 loading file C:\\Users\\altzh\\.flair\\models\\pos-english\\a9a73f6cd878edce8a0fa518db76f441f1cc49c2525b2b4557af278ec2f0659e.121306ea62993d04cd1978398b68396931a39eb47754c8a06a87f325ea70ac63\n",
      "2024-08-27 09:46:23,253 SequenceTagger predicts: Dictionary with 53 tags: <unk>, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD\n",
      "Sentence: \"The old man the boats .\" â†’ [\"The\"/DT, \"old\"/JJ, \"man\"/NN, \"the\"/DT, \"boats\"/NNS, \".\"/.]\n"
     ]
    }
   ],
   "source": [
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence\n",
    "\n",
    "# Load the pre-trained POS tagger (LSTM-based)\n",
    "tagger = SequenceTagger.load('pos')\n",
    "\n",
    "# Create a sentence object\n",
    "sentence = Sentence(\"The old man the boats.\")\n",
    "\n",
    "# Predict the POS tags\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# Print the tagged sentence\n",
    "print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2e84d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DET'), ('old', 'ADJ'), ('man', 'NOUN'), ('the', 'DET'), ('boats', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained BERT model for POS tagging\n",
    "model_name = \"vblagoje/bert-english-uncased-finetuned-pos\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Initialize the NLP pipeline for token classification (POS tagging)\n",
    "nlp = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Sentence to be tagged\n",
    "sentence = \"The old man the boats.\"\n",
    "\n",
    "# Tokenize and tag the sentence\n",
    "tagged_sentence = nlp(sentence)\n",
    "\n",
    "# Display the results\n",
    "out = []\n",
    "for entity in tagged_sentence:\n",
    "    out.append((entity['word'],entity['entity_group']))\n",
    "    #print(f\"{entity['word']}: {entity['entity_group']}\")\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30f32903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: The, POS: DET, Dependency: det, Head: man\n",
      "Token: old, POS: ADJ, Dependency: amod, Head: man\n",
      "Token: man, POS: NOUN, Dependency: ROOT, Head: man\n",
      "Token: the, POS: DET, Dependency: det, Head: boats\n",
      "Token: boats, POS: NOUN, Dependency: appos, Head: man\n",
      "Token: ., POS: PUNCT, Dependency: punct, Head: man\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# The tricky sentence\n",
    "sentence = \"The old man the boats.\"\n",
    "\n",
    "# Parse the sentence using spaCy\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Print out the dependencies and POS tags\n",
    "for token in doc:\n",
    "    print(f\"Token: {token.text}, POS: {token.pos_}, Dependency: {token.dep_}, Head: {token.head.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdd472f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\altzh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\altzh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('old', 'JJ'),\n",
       " ('man', 'NN'),\n",
       " ('the', 'DT'),\n",
       " ('boats', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "text = nltk.word_tokenize(SENTENCE)\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa150c64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
